{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tensorflow_addons","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tensorflow-gpu==2.2.0rc2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntf.__version__","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# import tensorflow as tf\nfrom tensorflow.keras.layers import *\nimport pandas as pd\nimport numpy as np\nimport random\nfrom tensorflow.keras.callbacks import Callback, LearningRateScheduler\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import losses, models, optimizers\nimport tensorflow_addons as tfa\n\nimport gc\nimport math\n\nfrom sklearn.model_selection import GroupKFold, KFold\nfrom sklearn.metrics import f1_score\n\nimport warnings\nwarnings.simplefilter('ignore')\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_rows', 500)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# configurations and main hyperparammeters\nEPOCHS = 100\nNNBATCHSIZE = 16\nGROUP_BATCH_SIZE = 4000\nSEED = 100\nLR = 0.0015\nSPLITS = 5\nORDER_OF_BATCHES = (0, 1, 2, 6, 5, 8, 3, 7, 4, 9)\nFOLDS_DIR = 'folds'\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def shift_signal(train, test):\n    SGNAL_SHIFT_CONSTANT = np.exp(1)\n    train.loc[2000_000:2500_000, 'signal'] += SGNAL_SHIFT_CONSTANT\n    train.loc[4500_000:, 'signal'] += SGNAL_SHIFT_CONSTANT\n\n    test.loc[500_000:600_000, 'signal'] += SGNAL_SHIFT_CONSTANT\n    test.loc[700_000:800_000, 'signal'] += SGNAL_SHIFT_CONSTANT\n    return train, test\n\n\ndef denoise_batch0_och0(df):\n    train = df.copy()\n    left = 47.857\n    right = 47.863\n    train['batch'] = train.index // 500_000\n    rnd_state = np.random.RandomState(100)\n    idxs_noisy = (train['batch']==0) & (train['open_channels']==0) & (train.time>left) & (train.time<right)\n    idxs_not_noisy = (train['batch']==0) & (train['open_channels']==0) & ~idxs_noisy\n    mu = train[idxs_not_noisy].signal.quantile(0.95)\n    sd = train[idxs_noisy].signal.std()\n    \n    size_idx = len(train[idxs_noisy].index)\n    jitter = 0.5*rnd_state.normal(mu, sd, size=size_idx)\n    train.loc[train[idxs_noisy].index, 'signal']  = mu + jitter\n    \n    tmp = train[idxs_noisy].copy()\n    MAX_PROBA_COL = tmp.filter(regex='proba').values.argmax(1)\n    for i in range(len(tmp)):\n        if tmp.loc[tmp.index[i], 'open_channels'] != MAX_PROBA_COL[i]:\n            true_col_idx = tmp.loc[tmp.index[i], 'open_channels']\n            pred_cod_idx = MAX_PROBA_COL[i]\n            # swapping probas\n            tmp.loc[tmp.index[i], f'proba_{true_col_idx}'], tmp.loc[tmp.index[i], f'proba_{pred_cod_idx}'] = \\\n            tmp.loc[tmp.index[i], f'proba_{pred_cod_idx}'], tmp.loc[tmp.index[i], f'proba_{true_col_idx}']\n\n    train[idxs_noisy] = tmp.values\n    train['open_channels'] = train['open_channels'].astype('int16')\n    del tmp, df\n    _ = gc.collect()\n\n    train.drop('batch', axis=1, inplace=True)\n    return train\n\n\ndef create_folds(train: pd.DataFrame):\n    train['batch'] = train.index // 500_000\n    if not os.path.exists(FOLDS_DIR):\n        os.makedirs(FOLDS_DIR)\n\n    for b in range(10):\n        temp = train.query(\"batch == @b\").reset_index(drop=True)\n        temp.to_csv(f\"{FOLDS_DIR}/f_{b}.csv\", index=False)\n        \n        \ndef swapping_batches(lst: list=ORDER_OF_BATCHES) -> pd.DataFrame:\n    df = pd.DataFrame()\n    for b in lst:\n        data = pd.read_csv(f'{FOLDS_DIR}/f_{b}.csv',\n                          dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n\n        df=df.append(data, ignore_index=True)\n    df.drop('batch', axis=1, inplace=True)\n    del data \n    _ = gc.collect()\n    return df\n\n\n# read data\ndef read_data():\n    \n    train = pd.read_csv('/kaggle/input/data-without-drift/train_clean.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n    test  = pd.read_csv('/kaggle/input/data-without-drift/test_clean.csv', dtype={'time': np.float32, 'signal': np.float32})\n    sub  = pd.read_csv('/kaggle/input/liverpool-ion-switching/sample_submission.csv', dtype={'time': np.float32})\n    \n    Y_train_proba = np.load(\"/kaggle/input/ion-shifted-rfc-proba/Y_train_proba.npy\")\n    Y_test_proba = np.load(\"/kaggle/input/ion-shifted-rfc-proba/Y_test_proba.npy\")\n    \n    for i in range(11):\n        train[f\"proba_{i}\"] = Y_train_proba[:, i]\n        test[f\"proba_{i}\"] = Y_test_proba[:, i]\n\n    return train, test, sub\n\n\ndef create_signal_mod(train: pd.DataFrame, power: float=0.5):\n    left = 3_641_000\n    right = 3_829_000\n    thresh_dict = {\n        3: [0.1, 2.0],\n        2: [-1.1, 0.7],\n        1: [-2.3, -0.6],\n        0: [-3.8, -2],\n    }\n    train['batch'] = train.index // 500_000\n    \n    train['signal_mod'] = train['signal'].values\n    rnd_state = np.random.RandomState(100)\n    for ch in train[train['batch']==7]['open_channels'].unique():\n        idxs_noisy = (train['open_channels']==ch) & (left<train.index) & (train.index<right)\n        idxs_not_noisy = (train['open_channels']==ch) & ~idxs_noisy\n        mu_upper = train[idxs_not_noisy]['signal'].quantile(0.75)\n        mu_low = train[idxs_not_noisy]['signal'].quantile(0.25)\n        sd = train[idxs_not_noisy]['signal'].std()\n\n        idxs_outlier_upper = idxs_noisy & (thresh_dict[ch][1]<train['signal'].values)\n        size_idx = len(train[idxs_outlier_upper])\n        jitter = power*rnd_state.normal(mu_upper, sd, size=size_idx)\n        train.loc[idxs_outlier_upper, 'signal_mod']  = mu_upper + jitter\n\n        idxs_outlier_low = idxs_noisy & (train['signal'].values<thresh_dict[ch][0])\n        size_idx = len(train[idxs_outlier_low])\n        jitter = power*rnd_state.normal(mu_low, sd, size=size_idx)\n        train.loc[idxs_outlier_low, 'signal_mod']  = mu_low + jitter\n    \n    train['signal'] = train['signal_mod'].values\n    train.drop(['batch', 'signal_mod'], axis=1, inplace=True)\n    \n    return train\n\n\ndef denois_train(df):\n    left = 3_641_000 #3_642_000\n    right = 3_829_000 #3_825_000\n    low = -3.8\n    upper = 2\n    train = df.copy()\n    nois_ind = train.loc[left:right, :].index\n    ind_to_nan = train.iloc[nois_ind][(train.signal>upper) | (train.signal<low)].index\n    \n    tmp = train[train.index.isin(ind_to_nan)].copy()\n    MAX_PROBA_COL = tmp.filter(regex='proba').values.argmax(1)\n    for i in range(len(tmp)):\n        if tmp.loc[tmp.index[i], 'open_channels'] != MAX_PROBA_COL[i]:\n            true_col_idx = tmp.loc[tmp.index[i], 'open_channels']\n            pred_cod_idx = MAX_PROBA_COL[i]\n            # swapping probas\n            tmp.loc[tmp.index[i], f'proba_{true_col_idx}'], tmp.loc[tmp.index[i], f'proba_{pred_cod_idx}'] = \\\n            tmp.loc[tmp.index[i], f'proba_{pred_cod_idx}'], tmp.loc[tmp.index[i], f'proba_{true_col_idx}']\n    \n    train[train.index.isin(ind_to_nan)] = tmp.values\n    train['open_channels'] = train['open_channels'].astype('int16')\n    del tmp\n    _ = gc.collect()\n    \n    return train\n\n\n# create batches of 4000 observations\ndef batching(df, batch_size):\n    df['group'] = df.groupby(df.index//batch_size, sort=False)['signal'].agg(['ngroup']).values\n    df['group'] = df['group'].astype(np.uint16)\n    return df\n\n# normalize the data (standard scaler). We can also try other scalers for a better score!\ndef normalize(train, test):\n    train_input_mean = train.signal.mean()\n    train_input_sigma = train.signal.std()\n    train['signal'] = (train.signal - train_input_mean) / train_input_sigma\n    test['signal'] = (test.signal - train_input_mean) / train_input_sigma\n    return train, test\n\n# get lead and lags features\ndef lag_with_pct_change(df, windows):\n    for window in windows:    \n        df['signal_shift_pos_' + str(window)] = df.groupby('group')['signal'].shift(window).fillna(0)\n        df['signal_shift_neg_' + str(window)] = df.groupby('group')['signal'].shift(-1 * window).fillna(0)\n    return df\n\n\ndef run_feat_engineering(df, batch_size):\n    # create batches\n    df = batching(df, batch_size = batch_size)\n    # create leads and lags (1, 2, 3 making them 6 features)\n    df = lag_with_pct_change(df, [1, 2, 3])\n    # create signal ** 2 (this is the new feature)\n    df['signal_2'] = df['signal'] ** 2\n    return df\n\n\ndef feature_selection(train, test):\n    features = [col for col in train.columns if col not in ['index', 'group', 'open_channels', 'time']]\n    train = train.replace([np.inf, -np.inf], np.nan)\n    test = test.replace([np.inf, -np.inf], np.nan)\n    for feature in features:\n        feature_mean = pd.concat([train[feature], test[feature]], axis = 0).mean()\n        train[feature] = train[feature].fillna(feature_mean)\n        test[feature] = test[feature].fillna(feature_mean)\n    return train, test, features\n\n\ndef Classifier(shape_):\n    \n    def cbr(x, out_layer, kernel, stride, dilation):\n        x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n        x = BatchNormalization()(x)\n        x = Activation(\"relu\")(x)\n        return x\n    \n    def wave_block(x, filters, kernel_size, n):\n        dilation_rates = [2**i for i in range(n)]\n        x = Conv1D(filters = filters,\n                   kernel_size = 1,\n                   padding = 'same')(x)\n        res_x = x\n        for dilation_rate in dilation_rates:\n            tanh_out = Conv1D(filters = filters,\n                              kernel_size = kernel_size,\n                              padding = 'same', \n                              activation = 'tanh', \n                              dilation_rate = dilation_rate)(x)\n            sigm_out = Conv1D(filters = filters,\n                              kernel_size = kernel_size,\n                              padding = 'same',\n                              activation = 'sigmoid', \n                              dilation_rate = dilation_rate)(x)\n            x = Multiply()([tanh_out, sigm_out])\n            x = Conv1D(filters = filters,\n                       kernel_size = 1,\n                       padding = 'same')(x)\n            res_x = Add()([res_x, x])\n        return res_x\n    \n    inp = Input(shape = (shape_))\n#     x = cbr(inp, 64, 7, 1, 1)\n#     x = BatchNormalization()(x)\n\n    x = wave_block(inp, 16, 3, 12)\n    x = BatchNormalization()(x)\n    x = wave_block(x, 32, 3, 8)\n    x = BatchNormalization()(x)\n    x = wave_block(x, 64, 3, 4)\n    x = BatchNormalization()(x)\n    x = wave_block(x, 128, 3, 1)\n#     x = cbr(x, 32, 7, 1, 1)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n    out = Dense(11, activation = 'softmax', name = 'out')(x)\n    \n    model = models.Model(inputs = inp, outputs = out)\n    \n    opt = Adam(lr = LR)\n    opt = tfa.optimizers.SWA(opt)\n    model.compile(loss = losses.CategoricalCrossentropy(), optimizer = opt, metrics = ['accuracy'])\n    return model\n\n\ndef lr_schedule(epoch):\n    if epoch < 30:\n        lr = LR\n    elif epoch < 40:\n        lr = LR / 3\n    elif epoch < 50:\n        lr = LR / 5\n    elif epoch < 60:\n        lr = LR / 7\n    elif epoch < 70:\n        lr = LR / 9\n    elif epoch < 80:\n        lr = LR / 11\n    elif epoch < 90:\n        lr = LR / 13\n    else:\n        lr = LR / 100\n    return lr\n\n# def step_decay(epoch):\n#     # Learning rate scheduler object\n#     initial_lrate = 0.001\n#     drop = 0.001\n#     epochs_drop = 3.0\n#     lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n#     return lrate\n\n# class to get macro f1 score. This is not entirely necessary but it's fun to check f1 score of each epoch (be carefull, if you use this function early stopping callback will not work)\nclass MacroF1(Callback):\n    def __init__(self, model, inputs, targets):\n        self.model = model\n        self.inputs = inputs\n        self.targets = np.argmax(targets, axis = 2).reshape(-1)\n        \n    def on_epoch_end(self, epoch, logs):\n        pred = np.argmax(self.model.predict(self.inputs), axis = 2).reshape(-1)\n        score = f1_score(self.targets, pred, average = 'macro')\n        print(f'F1 Macro Score: {score:.5f}')\n\n\ndef run_cv_model_by_batch(train, test, splits, batch_col, feats, sample_submission, nn_epochs, nn_batch_size):\n    \n    seed_everything(SEED)\n    K.clear_session()\n    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n    tf.compat.v1.keras.backend.set_session(sess)\n    oof_ = np.zeros((len(train), 11)) # build out of folds matrix with 11 columns, they represent our target variables classes (from 0 to 10)\n    preds_ = np.zeros((len(test), 11))\n    target = ['open_channels']\n    group = train['group']\n#     kf = KFold(n_splits=5, shuffle=False, random_state=100)\n#     splits = [x for x in kf.split(train, train[target])]\n\n    kf = GroupKFold(n_splits=splits)\n    splits = [x for x in kf.split(train, train[target], group)]\n\n    new_splits = []\n    for sp in splits:\n        new_split = []\n        new_split.append(np.unique(group[sp[0]]))\n        new_split.append(np.unique(group[sp[1]]))\n        new_split.append(sp[1])    \n        new_splits.append(new_split)\n    # pivot target columns to transform the net to a multiclass classification estructure (you can also leave it in 1 vector with sparsecategoricalcrossentropy loss function)\n    tr = pd.concat([pd.get_dummies(train.open_channels), train[['group']]], axis=1)\n\n    tr.columns = ['target_'+str(i) for i in range(11)] + ['group']\n    target_cols = ['target_'+str(i) for i in range(11)]\n    train_tr = np.array(list(tr.groupby('group').apply(lambda x: x[target_cols].values))).astype(np.float32)\n    train = np.array(list(train.groupby('group').apply(lambda x: x[feats].values)))\n    test = np.array(list(test.groupby('group').apply(lambda x: x[feats].values)))\n\n    for n_fold, (tr_idx, val_idx, val_orig_idx) in enumerate(new_splits[0:], start=0):\n        train_x, train_y = train[tr_idx], train_tr[tr_idx]\n        valid_x, valid_y = train[val_idx], train_tr[val_idx]\n        print(f'Our training dataset shape is {train_x.shape}')\n        print(f'Our validation dataset shape is {valid_x.shape}')\n        \n#         # weights\n#         w = (train.shape[0] / (len(np.bincount(train.open_channels.values))*np.bincount(train.open_channels.values)))\n#         weights = dict(zip(np.arange(11), w))\n#         train_weights = train.open_channels.map(weights).values\n\n        gc.collect()\n        shape_ = (None, train_x.shape[2]) # input is going to be the number of feature we are using (dimension 2 of 0, 1, 2)\n        model = Classifier(shape_)\n        # using our lr_schedule function\n        cb_lr_schedule = LearningRateScheduler(lr_schedule)\n        model.fit(train_x,train_y,\n#                   sample_weight=train_weights,\n                  epochs = nn_epochs,\n                  callbacks = [cb_lr_schedule, MacroF1(model, valid_x, valid_y)], # adding custom evaluation metric for each epoch\n                  batch_size = nn_batch_size,verbose = 2,\n                  validation_data = (valid_x,valid_y))\n        preds_f = model.predict(valid_x)\n        f1_score_ = f1_score(np.argmax(valid_y, axis=2).reshape(-1),  np.argmax(preds_f, axis=2).reshape(-1), average = 'macro') # need to get the class with the biggest probability\n        print(f'Training fold {n_fold + 1} completed. macro f1 score : {f1_score_ :1.5f}')\n        preds_f = preds_f.reshape(-1, preds_f.shape[-1])\n        oof_[val_orig_idx,:] += preds_f\n        te_preds = model.predict(test)\n        te_preds = te_preds.reshape(-1, te_preds.shape[-1])           \n        preds_ += te_preds / SPLITS\n    # calculate the oof macro f1_score\n    f1_score_ = f1_score(np.argmax(train_tr, axis = 2).reshape(-1),  np.argmax(oof_, axis = 1), average = 'macro') # axis 2 for the 3 Dimension array and axis 1 for the 2 Domension Array (extracting the best class)\n    print(f'Training completed. oof macro f1 score : {f1_score_:1.5f}')\n    sample_submission['open_channels'] = np.argmax(preds_, axis = 1).astype(int)\n    sample_submission.to_csv(f'submission_wavenet_{f1_score_:0.5f}.csv', index=False, float_format='%.4f')\n    \n# run entire program\ndef run_everything():\n    \n    print('Reading Data Started...')\n    train, test, sample_submission = read_data()\n    print('denois_train run...')\n    train = denois_train(train)\n    print('denoise_batch0_och0')\n    train = denoise_batch0_och0(train)\n    print('create_signal_mod run...')\n    train = create_signal_mod(train)\n#     print('shift_signal')\n#     train, test = shift_signal(train, test)\n    \n    train, test = normalize(train, test)\n    print('Reading and Normalizing Data Completed')\n    \n#     print('Creating folds')\n#     create_folds(train)\n#     print('swapping batches')\n#     train = swapping_batches()\n    \n    print('Creating Features')\n    print('Feature Engineering Started...')\n    train = run_feat_engineering(train, batch_size = GROUP_BATCH_SIZE) \n    test = run_feat_engineering(test, batch_size = GROUP_BATCH_SIZE)\n    train, test, features = feature_selection(train, test)\n    print('Feature Engineering Completed...')\n        \n   \n    print(f'Training Wavenet model with {SPLITS} folds of GroupKFold Started...')\n    run_cv_model_by_batch(train, test, SPLITS, 'group', features, sample_submission, EPOCHS, NNBATCHSIZE)\n    print('Training completed...')\n        \nrun_everything()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}